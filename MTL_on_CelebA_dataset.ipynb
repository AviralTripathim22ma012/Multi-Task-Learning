{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOus37Xap6QBQMuk0YjrCr0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AviralTripathim22ma012/Multi-Task-Learning/blob/main/MTL_on_CelebA_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MULTI-TASK LEARNING WITH 8 ATTRIBUTES**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SiZG8ANzql_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CelebA\n",
        "from torchvision import transforms, models\n",
        "\n",
        "'''# Define device'''\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "'''# Define transforms'''\n",
        "transform = transforms.Compose([\n",
        "    transforms.CenterCrop(178),\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "'''# Define dataset and dataloader'''\n",
        "train_dataset = CelebA(root=\"./data\", split=\"train\", target_type=\"attr\", transform=transform, download=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "'''# Define model'''\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 4096),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096, 4096),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096, 8),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "'''# Define loss function and optimizer'''\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "'''# Train the model'''\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs, labels = inputs.to(device), labels[:, :8].to(device)  # Only use 8 attributes\n",
        "        optimizer.zero_grad()\n",
        "q        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(\"Epoch %d loss: %.3f\" % (epoch + 1, running_loss / len(train_dataloader)))\n",
        "\n",
        "'''# Test the model'''\n",
        "test_dataset = CelebA(root=\"./data\", split=\"test\", target_type=\"attr\", transform=transform, download=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "task_accs = [0.0] * 8\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels[:, :8].to(device)  # Only use 8 attributes\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > 0.5).float()\n",
        "        for i in range(8):\n",
        "            task_accs[i] += (preds[:, i] == labels[:, i]).float().sum().item()\n",
        "        correct += (preds == labels).float().sum().item()\n",
        "        total += labels.numel()\n",
        "task_accs = [task_acc / total for task_acc in task_accs]\n",
        "overall_acc = correct / total\n",
        "\n",
        "'''# Report task-wise accuracy and overall accuracy'''\n",
        "print(\"Task-wise accuracy:\")\n",
        "for i in range(8):\n",
        "    print(\"Attribute %d: %.3f\" % (i + 1, task_accs[i]))\n",
        "print(\"Overall accuracy: %.3f\" % overall_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDk0TUZwqKSJ",
        "outputId": "226159e6-9835-4aaf-feef-96bd39f8562d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1443490838it [00:25, 56236993.95it/s]\n",
            "26721026it [00:00, 63401324.43it/s]\n",
            "3424458it [00:00, 200999424.67it/s]\n",
            "6082035it [00:00, 21752765.99it/s]\n",
            "12156055it [00:00, 35414380.68it/s]\n",
            "2836386it [00:00, 201167864.07it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:04<00:00, 128MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 0.349\n",
            "Epoch 2 loss: 0.300\n",
            "Epoch 3 loss: 0.291\n",
            "Epoch 4 loss: 0.286\n",
            "Epoch 5 loss: 0.282\n",
            "Epoch 6 loss: 0.279\n",
            "Epoch 7 loss: 0.278\n",
            "Epoch 8 loss: 0.275\n",
            "Epoch 9 loss: 0.274\n",
            "Epoch 10 loss: 0.273\n",
            "Files already downloaded and verified\n",
            "Task-wise accuracy:\n",
            "Attribute 1: 0.117\n",
            "Attribute 2: 0.103\n",
            "Attribute 3: 0.102\n",
            "Attribute 4: 0.105\n",
            "Attribute 5: 0.123\n",
            "Attribute 6: 0.119\n",
            "Attribute 7: 0.088\n",
            "Attribute 8: 0.105\n",
            "Overall accuracy: 0.863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DROP RATE (Bonus)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qwzeI4Ge77Tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have chosen the **2nd matrix (based on the number of labels per attribute)** to calculate the drop rate, here are the steps I followed:\n",
        "\n",
        "\n",
        "1.   **•\tSTEP-1:** Get the names of first 8 attributes from the celebA dataset, along with the number of corresponding labels  \n",
        "2.   **•\tSTEP-2:** Find the maximum amongst these number of labels and divide all the other attributes with this maximum value, and print the result\n",
        "1.   **•\tSTEP-3:** Find the average of these ratios and print only those attributes which are grater than this average\n",
        "2.   **•\tSTEP-4:** Get the corresponding indices of the attributes obtained in the previous step and train the model only for those attributes  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cl7UuRu47LQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRINTING FIRST 8 ATTRIBUTES**"
      ],
      "metadata": {
        "id": "sIr4w0_m50R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CelebA\n",
        "\n",
        "'''# Load the CelebA dataset'''\n",
        "celeba_dataset = CelebA(root='./data', split='train', target_type='attr', download=True)\n",
        "\n",
        "'''# Get the attribute names and print the first 8'''\n",
        "attr_names = celeba_dataset.attr_names[:8]\n",
        "print(attr_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNeQK46eDqs9",
        "outputId": "8ad540cc-6a56-458b-9afc-b98e650b53f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NUMBER OF LABELS PER ATTRIBUTE**"
      ],
      "metadata": {
        "id": "hA1ky4z258_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attr_names = celeba_dataset.attr_names\n",
        "attr_indices = [celeba_dataset.attr_names.index(attr_name) for attr_name in ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose']]\n",
        "\n",
        "'''# Get the corresponding number of labels for each attribute'''\n",
        "num_labels = [celeba_dataset.attr[:, attr_idx].sum().item() for attr_idx in attr_indices]\n",
        "\n",
        "'''# Print the results'''\n",
        "for i in range(len(attr_names)):\n",
        "    if i in attr_indices:\n",
        "        attr_idx = attr_indices.index(i)\n",
        "        print(f\"{attr_names[i]}: {num_labels[attr_idx]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LppQxxSQ_zii",
        "outputId": "43cc6f41-cfdb-4f2b-e3cc-140e160c3011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5_o_Clock_Shadow: 18177\n",
            "Arched_Eyebrows: 43278\n",
            "Attractive: 83603\n",
            "Bags_Under_Eyes: 33280\n",
            "Bald: 3713\n",
            "Bangs: 24685\n",
            "Big_Lips: 39213\n",
            "Big_Nose: 38341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DIVIDING BY MAXIMUM NUMBER OF LABELS**"
      ],
      "metadata": {
        "id": "hmk9w46Y6EKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Find the maximum value among the number of labels'''\n",
        "max_num_labels = max(num_labels)\n",
        "\n",
        "'''# Divide each number of labels with the maximum value'''\n",
        "normalized_labels = [num_labels[i] / max_num_labels for i in range(len(num_labels))]\n",
        "\n",
        "'''# Print the results'''\n",
        "for i in range(len(attr_names)):\n",
        "    if i in attr_indices:\n",
        "        attr_idx = attr_indices.index(i)\n",
        "        print(f\"{attr_names[i]}: {normalized_labels[attr_idx]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QPx6xjfABgB",
        "outputId": "6ecdeb79-04b9-4982-aee0-f8c0af753e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5_o_Clock_Shadow: 0.21742042749662094\n",
            "Arched_Eyebrows: 0.517660849491047\n",
            "Attractive: 1.0\n",
            "Bags_Under_Eyes: 0.3980718395272897\n",
            "Bald: 0.04441228185591426\n",
            "Bangs: 0.2952645240003349\n",
            "Big_Lips: 0.4690381924093633\n",
            "Big_Nose: 0.45860794469098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SELECTING THOSE WHICH ARE GRATER THAN THE AVERAGE (AND DROPING REST)**"
      ],
      "metadata": {
        "id": "YapPLFqd6K-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Find the average of the normalized values'''\n",
        "avg_normalized_labels = sum(normalized_labels) / len(normalized_labels)\n",
        "\n",
        "'''# Print the normalized values that are higher than the average'''\n",
        "for i in range(len(attr_names)):\n",
        "    if i in attr_indices:\n",
        "        attr_idx = attr_indices.index(i)\n",
        "        if normalized_labels[attr_idx] > avg_normalized_labels:\n",
        "            print(f\"{attr_names[i]}: {normalized_labels[attr_idx]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in-b4MUfBA_i",
        "outputId": "6a5d9c03-9460-49b9-ffb5-73bd4a337563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arched_Eyebrows: 0.517660849491047\n",
            "Attractive: 1.0\n",
            "Big_Lips: 0.4690381924093633\n",
            "Big_Nose: 0.45860794469098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Get the attribute names and indices'''\n",
        "attr_names = celeba_dataset.attr_names\n",
        "attr_indices = [attr_names.index(attr_name) for attr_name in ['Arched_Eyebrows', 'Attractive', 'Bangs', 'Big_Nose']]\n",
        "\n",
        "'''# Print the indices'''\n",
        "print(attr_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjIlEjl4FHxe",
        "outputId": "817f0839-020c-4d46-9658-9a2a05db45b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 5, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REPEATING THE TRAINING WITH ONLY 4 TASKS**"
      ],
      "metadata": {
        "id": "2alIks-J6Uzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqKfSsaHHEO7",
        "outputId": "c2efa726-a101-4b5f-e112-8dc992c13da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1443490838it [00:21, 67038929.71it/s] \n",
            "26721026it [00:00, 149507568.66it/s]\n",
            "3424458it [00:00, 223225442.73it/s]\n",
            "6082035it [00:00, 68389879.36it/s]\n",
            "12156055it [00:00, 104893668.90it/s]\n",
            "2836386it [00:00, 199568294.06it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:08<00:00, 68.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 0.574\n",
            "Epoch 2 loss: 0.562\n",
            "Epoch 3 loss: 0.562\n",
            "Epoch 4 loss: 0.562\n",
            "Epoch 5 loss: 0.562\n",
            "Epoch 6 loss: 0.561\n",
            "Epoch 7 loss: 0.561\n",
            "Epoch 8 loss: 0.561\n",
            "Epoch 9 loss: 0.561\n",
            "Epoch 10 loss: 0.561\n",
            "Files already downloaded and verified\n",
            "Task-wise accuracy:\n",
            "Attribute 1: 0.179\n",
            "Attribute 2: 0.124\n",
            "Attribute 3: 0.211\n",
            "Attribute 4: 0.197\n",
            "Overall accuracy: 0.711\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CelebA\n",
        "from torchvision import transforms, models\n",
        "\n",
        "'''# Define device'''\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "'''# Define transforms'''\n",
        "transform = transforms.Compose([\n",
        "    transforms.CenterCrop(178),\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "'''# Define dataset and dataloader'''\n",
        "train_dataset = CelebA(root=\"./data\", split=\"train\", target_type=\"attr\", transform=transform, download=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "'''# Define model'''\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 4096),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096, 4096),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096, 4),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "'''# Define loss function and optimizer'''\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "'''# Train the model'''\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "        inputs, labels = inputs.to(device),  labels[:, [1, 2, 5, 7]].to(device)  # Only use 4 attributes\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(\"Epoch %d loss: %.3f\" % (epoch + 1, running_loss / len(train_dataloader)))\n",
        "\n",
        "'''# Test the model'''\n",
        "test_dataset = CelebA(root=\"./data\", split=\"test\", target_type=\"attr\", transform=transform, download=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "task_accs = [0.0] * 4\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels[:, [1, 2, 5, 7]].to(device)  # Only use 4 attributes\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > 0.5).float()\n",
        "        for i in range(4):\n",
        "            task_accs[i] += (preds[:, i] == labels[:, i]).float().sum().item()\n",
        "        correct += (preds == labels).float().sum().item()\n",
        "        total += labels.numel()\n",
        "task_accs = [task_acc / total for task_acc in task_accs]\n",
        "overall_acc = correct / total\n",
        "\n",
        "'''# Report task-wise accuracy and overall accuracy'''\n",
        "print(\"Task-wise accuracy:\")\n",
        "for i in range(4):\n",
        "    print(\"Attribute %d: %.3f\" % (i + 1, task_accs[i]))\n",
        "print(\"Overall accuracy: %.3f\" % overall_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MULTI-TASKING WITH THESE 4 AS WELL AS THE OTHERS**"
      ],
      "metadata": {
        "id": "kQnEA1GX6eX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, datasets, transforms\n",
        "\n",
        "'''# Define the multi-task model with task dropout'''\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, num_tasks, task_sizes):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        self.num_tasks = num_tasks\n",
        "        self.task_sizes = task_sizes\n",
        "        self.features = nn.Sequential(*list(models.vgg16(pretrained=True).features.children())[:-1])\n",
        "        self.layers = nn.ModuleList()\n",
        "        for size in task_sizes:\n",
        "            self.layers.append(nn.Linear(4096, size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x).view(x.size(0), -1)\n",
        "        outputs = []\n",
        "        for i in range(self.num_tasks):\n",
        "            if i not in [0, 1, 4, 6]:  # Drop tasks 1, 2, 5, and 7\n",
        "                output = self.layers[i](x)\n",
        "                outputs.append(output)\n",
        "        return tuple(outputs)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.CenterCrop(178),\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "\n",
        "'''# Load the data for each task'''\n",
        "train_data = datasets.CelebA('./data', split='train', transform=transform, download= True)\n",
        "attr_indices = [0, 1, 4, 6]  # Attributes to predict (tasks 1, 2, 5, and 7 are dropped)\n",
        "targets = torch.stack([train_data.attr[i] for i in attr_indices], dim=1)\n",
        "\n",
        "'''# Create data loaders'''\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "'''# Create an instance of the multi-task model'''\n",
        "num_tasks = targets.shape[1]\n",
        "task_sizes = [2] * num_tasks\n",
        "model = MultiTaskModel(num_tasks, task_sizes)\n",
        "\n",
        "'''# Define the loss functions for each task'''\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "'''# Define the optimizer'''\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "'''# Train the model with task dropout'''\n",
        "num_epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, (images, _) in enumerate(test_loader):\n",
        "        # Move the data to the device\n",
        "        images = images.to(device)\n",
        "        idx = i % len(test_loader.batch_sampler)\n",
        "        targets_batch = targets[test_loader.batch_sampler[idx]].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = sum([criterion(outputs[i], targets_batch[:, i]) for i in range(len(outputs))])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        test_loss += loss\n",
        "\n",
        "    print(\"Epoch %d loss: %.3f\" % (epoch + 1, running_loss / len(train_dataloader)))\n",
        "\n",
        "    # Calculate the average train loss\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "'''# Evaluate the model on the validation set'''\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for i, (images, _) in enumerate(test_loader):\n",
        "        # Move the data to the device\n",
        "        images = images.to(device)\n",
        "        targets_batch = targets[list(test_loader.batch_sampler.sampler)[i * batch_size:(i+1) * batch_size]].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = sum([criterion(outputs[i], targets_batch[:, i]) for i in range(len(outputs))])\n",
        "\n",
        "\n",
        "        preds = (outputs > 0.5).float()\n",
        "        for i in range(8):\n",
        "            task_accs[i] += (preds[:, i] == labels[:, i]).float().sum().item()\n",
        "        correct += (preds == labels).float().sum().item()\n",
        "        total += labels.numel()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        test_loss += loss\n",
        "\n",
        "    # Calculate the average validation loss\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "\n",
        "overall_acc = correct / total\n",
        "print(\"Overall accuracy: %.3f\" % overall_acc)"
      ],
      "metadata": {
        "id": "5rUiS7sYkh8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca73f759-ff14-48f0-f5bc-a57f2f159bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 loss: 0.256\n",
            "epoch 2 loss: 0.245\n",
            "epoch 3 loss: 0.233\n",
            "epoch 4 loss: 0.222\n",
            "epoch 5 loss: 0.21\n",
            "epoch 6 loss: 0.197\n",
            "epoch 7 loss: 0.195\n",
            "epoch 8 loss: 0.194\n",
            "epoch 9 loss: 0.192\n",
            "epoch 10 loss: 0.192\n",
            "overall accuracy: 0.914 \n"
          ]
        }
      ]
    }
  ]
}